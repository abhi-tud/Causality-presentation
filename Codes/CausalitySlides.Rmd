---
title: "Causality"
subtitle: "In reference to NICK HUNTINGTON-KLEIN"
author: "Abhisek Banerjee"
institute: "University of Duisburg Essen"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default", "ude_fonts.css", "ude.css"]
    self_contained: true # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "mathjax-equation-numbers.html"
    nature:
      beforeInit: ["remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' #alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: TRUE #disable slide transitions by scrolling
---
##Motivation

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```
<br>
Before going into all the technical stuff about causality, let's try to understand why do we need to worry so much about causality?
</br>

--


Because many research questions we are interested in are causal in nature.
- We don’t want to know if countries with higher minimum wages have less poverty, we want to know if raising the minimum wage reduces poverty. 
- We don’t want to know if people who take a popular common-cold-shortening medicine get better, we want to know if the medicine made them get better more quickly. 
- We don’t want to know if the central bank cutting interest rates was shortly followed by a recession, we want to know if the interest rate cut caused the recession.


---
##SO WHAT IS CAUSALITY?

- We say that X causes Y if…
- were we to intervene and change the value of X without changing anything else…
- then Y would also change as a result


--

###Examples of causality:

- Obvious ones:
 - A light switch being set to on causes the light to be on
 - Setting off fireworks raises the noise level
- less obvious ones:
 - Getting a college degree increases your earnings
 - Tariffs reduce the amount of trade

---

#Some more Examples..


Here are some examples which have non-zero correlation but are not causal (or may be causal in the wrong direction!)

- Obvious ones:
 - People tend to wear shorts on days when ice cream trucks are out
 - Rooster crowing sounds are followed closely by sunrise

- Less Obvious ones:
 - Colds tend to clear up a few days after you take Emergen-C
 - The performance of the economy tends to be lower or higher depending on the president’s political party
 

---
##Casusal Inference

- There are plenty of correlations that aren’t causal. "X causes Y” doesn’t mean that X is necessarily the only thing that causes Y. It also doesn’t mean that all Y must be X


--

For example,

- let us consider a light switch that causes the light to go on. 

- However, it cannot turn of the bulb if the bulb is burned out (no Y, despite X), or if the light was already on (Y without X), and it ALSO needs electicity(something else causes Y)


- But still we’d say that using the switch causes the light! The important thing is that X changes the distribution of Y, not that it necessarily makes it happen for certain


  
- So, if we encounter a correlation and wonder about causality, then we first need to think on Causal Inference.

**Causal Inference is inferring causality from data**


---
##Definition of Causality
**There is no single definition for causality**
.content-box-red[
However, we can say that X cause Y if, were we to intervene and change the value of X, then the distribution of Y aould also change as a result.
]

This definition lets us distinguish between correlation and causation.

- For example, we can observe that the number of people who wear shorts is much higher on days when people eat ice cream. However, if we were to intervene and swap out someone’s pants for shorts, would it make them more likely to eat ice cream? Probably not! So this is a non-causal relationship.

This definition very important in social science, where very little is ever for certain. 


---
##Weasel words
- There are lots of words that are generally taken to imply causality, as well as words that describe relationships without implying causality. 

-There are also some “weasel words” that don’t technically say anything about causality but clearly really want you to hear it.


<br>
.center[What are some of these words?]
</br>

--

We can say X causes Y by saying:
- X causes Y
- X affects Y
- The effect of X on Y
- so on..


---
##Why are Weasel words important?

- Knowing these terms can help you interpret what scientific studies are really saying, and when someone might be trying to pull one over on you.

- Also, looking at these words might help us figure out what exactly causality is.The causal phrases have direction. They tell us that X is doing to Y.In contrast, the clearly non-causal terms don’t even need to specify which of X and Y goes first. They just talk about these two variables and how they work together. The weasel terms are so weaselly specifically because they’re written in a way that implies that direction from X to Y, even if they’re not literally claiming a causal relationship.

.footnote[
The key is that all the weasel phrases are equally true if you swap the positions of X and Y, even though swapping them would really change how we’d interpret the claim. “Aspirin is linked to headaches” is no more or less true than “Headaches are linked to aspirin.” You can hear the notion that aspirin causes your headache in the first one, which is false. You can get away with it without technically lying because it’s true that they’re “linked.”
]

---
##The problem of Causal Inference
- Let’s try to think about whether some X causes Y. That is, if we manipulated X, then Y would change as a result. 

- Let’s consider just one person - Angela. We could just check what Angela’s Y is when we make X=0(she did not get a medical treatment), and then check what Angela’s Y is again when we make X=1(she got a medical treatment).

- If the two Y's are different, then we can say that X causes Y.

- However, Angela can’t exist both with X=0 and with X=1. She either got that medical treatment or she didn’t.

- Let’s say she did. So for Angela, X=1 and, let’s say, Y=10.
The other one, what Y would have been if we made X=0, is missing. We don’t know what it is! Could also be Y=10. Could be Y=9. Could be Y=1000!

---
##A possible solution!

- We can take someone who actually DOES have X=0 and compare their Y?

- However, as both of them are different people, it is quite natural that they will have different Y BESIDES X. 

- So if we find someone, Gareth, with X=0 and they have Y=9, is that because X increases Y, or is that just because Angela and Gareth would have had different Ys anyway?

- In order to overcome with this problem, and making as good a guess as possible as to what that Y would have been if X had been different, we want to think about two people/firms/countries that are basically **exactly the same** except that one has X=0 and one has X=1
</br>

---
##POTENTIAL OUTCOMES
<br>
The logic we just went through is the basis of the potential outcomes model, which is one way of thinking about causality.
</br>
<br>
Figuring out that makes a good counterfactual estimate is a key part of causal inference!
</br>
.footnote[
counterfactual - counter to the fact of what actually happened]

---
##Experiments and Models
- A common way to do causal inference in many fields is an experiment.
- When we’re working with people/firms/countries, running experiments is often infeasible, impossible, or unethical.
- So we have to think hard about a model of what the world looks like.
- So that we can use our model to figure out what the counterfactual would be.
- In causal inference, the model is our idea of what we think the process is that generated the data.
- We put together what we know about the world with assumptions we make and end up with our model.
-The model can then tell us what kinds of things could give us wrong results so we can fix them and get the right counterfactual.

---
#Model Identification
- We have “identified” a causal effect if the estimate that we generate gives us a causal effect.
- In other words, when we see the estimate, we can claim that it’s isolating just the causal effect.
- Identifying effects requires us to understand the data generating process (DGP).
- And once we understand that DGP, knowing what calculations we need to do to isolate our effect.Often these will require taking some conditional values (controls). Or isolating the variation we want in some otherway.
- We need to think about how to create models of the processes that generated the data.
- Once we have those models, we’ll figure out what methods we can use to generate plausible counterfactuals.
- Once we’re really comparing apples to apples, we can figure out, using only data we can actually observe, how things would be different if we reached in and changed X, and how Y would change as a result.

---
##Causal Diagrams
.content-box-red[
A causal diagram is a graphical representation of a data generating process (DGP).
]

- Causal diagrams were developed in the mid-1990s by the computer scientist Judea Pearl (2009Pearl, Judea. 2009. Causality. 2nd ed. Cambridge, MA: Cambridge University Press.), who was trying to develop a way for artificial intelligence to think about causality. 

- A causal diagram contains only two things:
 - The variables in the DGP, each represented by a node on the diagram.
 - The causal relationships in the DGP, each represented by an arrow from the cause variable to the caused variable.

---
##A simple Example
- Two people flips a coin. 
- If they get head, then person A gets a slice of cake. 
- If thex get tail, then person B gets the slice.
- Here, we have two variables 
 - the outcome of coin flip
 - which person gets the cake
- We have one causal relationship
 - the outcome of the coin flip determines which person got cake.
`
```{r, include=FALSE}
library(ggdag)
library(ggplot2)

```
 
```{r, eval=TRUE,echo=FALSE,fig.height=2}
coord_dag <- list(
  x = c(a = 3, b = 1),
  y = c(a = 1, b = 1)
)
dag_object <- ggdag::dagify(a ~ b,
                            
                            coords = coord_dag,
                            labels=c("a"="Cake",
                                     "b"="Coin flip"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()

```
 
---
##Points to be noted:
- Each variable on the graph may take multiple values. 

- The arrow just tells us that one variable causes another. It doesn’t say anything about whether that causal effect is positive or negative.

- Other events that might effect the outcomes are ignores.

- Outcomes of outcomes are not taken into consideration.

- All (non-trivial) variables relevant to the data generating process should be included, even if we can’t measure or see them.

---
###Unmeasured Variables
- Serve two purposes in causal diagrams:
 - they are the key parts of the data generating process
 - hey can sometimes fill in for variables that we know must be there, but we have no idea what they are.
--

###Latent variables
- They are an example of unmeasured variables.
- Explains why two variables that are correlated but neither of them causes the other. 
- We can put them in diagram lebelled as "L" or "U".

---
##An Example:

When people wear Shorts, there is a high sale of Ice-Cream. However, there is no correlation between , sales of Ice-cream and people wearing sorts.A latent variable doing causing them both.

```{r, eval=TRUE,echo=FALSE,fig.height=3}
coord_dag <- list(
  x = c(a = 1, b = 3, c = 5),
  y = c(a = 1, b = 2, c = 1)
)
dag_object <- ggdag::dagify(a ~ b,
                            c ~ b,
                            coords = coord_dag,
                            labels=c("a"="Shorts",
                                     "b"="U",
                                     "c"="Ice-Cream"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()
```


---
```{r,include=FALSE}
library(wooldridge)
library(tidyverse)
library(extrafont)
library(ggpubr)
library(ggplot2)
```
##The Real World: On an Omission Mission
.pull-left[
- The following figure shows: Police Presence and Crime Rate by County, North Carolina 1981-1987.
- The graph implies: Additional police presence is consistently associated with more crime.
- The raw correlation is picking up the fact that high-crime areas get assigned lots of police.]

]

.pull-right[
```{r, eval=TRUE,echo=FALSE,fig.height=3}
data(crime4)
ggplot(crime4, aes(x = polpc, y = crmrte)) + 
  geom_point() + 
  theme_pubr() + 
  scale_x_log10() + 
  scale_y_log10() + 
  theme(text         = element_text(size = 13 ),
        axis.title.x = element_text(size = 13 ),
        axis.title.y = element_text(size = 13)) +
  labs(x = "per Capita Police (log scale)",
       y = "Crime Rate (log scale)")
```
]

---
##Causal diagram

- To make a causal diagram, we think of following latent varaibles:
 - The unobservable ExpectedCrimePayout
 - It is based on how many police there are and sentencing law
- Sentencing laws and police per capita are also caused by LawAndOrderPolitics
- Some assumptions are:
 - LaggedCrime doesn’t cause LawAndOrderPolitics
 - PovertyRate isn’t a part of the data generating process
 - LaggedPolicePerCapita doesn’t cause PolicePerCapita 
 - RecentPopularCrimeMovie doesn’t cause Crime 

---
##The Diagram:
The causal diagram looks like:


```{r, eval=TRUE,echo=FALSE,fig.height=6}
coord_dag <- list(
  x = c(a = 1, b = 4, c = 4, d = 7, e = 1, f = 7),
  y = c(a = 1, b = 5, c = 3, d = 1, e = 5, f = 5)
)
dag_object <- ggdag::dagify(b ~ a,
                            c ~ a,
                            d ~ a,
                            d ~ c,
                            f ~ c,
                            c ~ e,
                            b ~ e,
                            f ~ b,
                            f ~ d,
                            coords = coord_dag,
                            labels=c("a"="Lagged crime",
                                     "b"="Sentencing Laws",
                                     "c"="Police per Capita",
                                     "d"="Crime",
                                     "e"="Law and Order politics",
                                     "f"="Expected Crime Payouts"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()
```
]
---
##Research Questions in Causal Diagrams
- We should use our causal diagram to figure out how to identify the answer to our research question.

For instance, let us consider the police-and-crime figure again.

**Question** “does additional police presence reduce crime?”

--

- Looking at this diagram, we should be able to see which parts of the diagram answer our research question of interest.(any parts of it that allow PolicePerCapita to cause Crime)
- We have:
 - PolicePerCapita → Crime (direct effect) andExpectedCrimePayout → Crime (indirect effect)
- Ask yourself: if the reason that police presence reduces crime is because criminals commit less crime when they expect to be caught 
- The variation in our data that answers our research question has to do with PolicePerCapita causing Crime, and to do with PolicePerCapita causing ExpectedCrimePayout, which then affects Crime. 
- To identify our answer, we have to dig out that part of the variation and block out the alternative explanations.

---
##Moderators in Causal Diagrams
.content-box-red[
Moderators are variables that don’t necessarily cause another variable (although they might do that too). Instead, they modify the effect of one variable on another.
] 
- Example:consider the effect of a fertility drug (X) on the chances of getting pregnant (Y). The effect is moderated by the variable “having a uterus” (Z). If you don’t have a uterus, the drug can’t do much for you. But if you do have a uterus, it can increase your chances of conceiving.
 
---
##Moderators on Causal Diagram:
- Technically, the moderator effects are not shown on the causal diagram.
Let us consider the causal diagram to the right:
- It implies X → Y and Z → Y
- However, this could be consistent with any of the following data generating processes:
 - Y = .2X + .3Z
 - Y= 2X + 3XZ , & Infinite many more ..
- Z has a moderating influence on the effect of X.

```{r, eval=TRUE,echo=FALSE,fig.height=3}
coord_dag <- list(
  x = c(a = 1, b = 3, c = 5),
  y = c(a = 2, b = 1, c = 2)
) 
dag_object <- ggdag::dagify(b ~ a,
                            b ~ c,
                            coords = coord_dag,
                            labels=c("a"="X",
                                     "b"="Y",
                                     "c"="Z"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()
```

---

